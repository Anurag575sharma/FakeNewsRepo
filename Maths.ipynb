{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "596e0165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\tamra\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\tamra\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\tamra\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tamra\\anaconda3\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\tamra\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Collecting scikit-fuzzy\n",
      "  Downloading scikit_fuzzy-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.25.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\tamra\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Downloading scikit_fuzzy-0.5.0-py2.py3-none-any.whl (920 kB)\n",
      "   ---------------------------------------- 0.0/920.8 kB ? eta -:--:--\n",
      "   ---------------------------------- ----- 786.4/920.8 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 920.8/920.8 kB 8.4 MB/s eta 0:00:00\n",
      "Installing collected packages: scikit-fuzzy\n",
      "Successfully installed scikit-fuzzy-0.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn pandas numpy matplotlib tensorflow scikit-fuzzy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19d740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tamra\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import skfuzzy as fuzz\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5c776fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2635.json        FALSE  \\\n",
      "0  10540.json    half-true   \n",
      "1    324.json  mostly-true   \n",
      "2   1123.json        FALSE   \n",
      "3   9028.json    half-true   \n",
      "4  12465.json         TRUE   \n",
      "\n",
      "  Says the Annies List political group supports third-trimester abortions on demand.  \\\n",
      "0  When did the decline of coal start? It started...                                   \n",
      "1  Hillary Clinton agrees with John McCain \"by vo...                                   \n",
      "2  Health care reform legislation is likely to ma...                                   \n",
      "3  The economic turnaround started at the end of ...                                   \n",
      "4  The Chicago Bears have had more starting quart...                                   \n",
      "\n",
      "                             abortion    dwayne-bohac  \\\n",
      "0  energy,history,job-accomplishments  scott-surovell   \n",
      "1                      foreign-policy    barack-obama   \n",
      "2                         health-care    blog-posting   \n",
      "3                        economy,jobs   charlie-crist   \n",
      "4                           education       robin-vos   \n",
      "\n",
      "         State representative      Texas  republican     0     1    0.1  \\\n",
      "0              State delegate   Virginia    democrat   0.0   0.0    1.0   \n",
      "1                   President   Illinois    democrat  70.0  71.0  160.0   \n",
      "2                         NaN        NaN        none   7.0  19.0    3.0   \n",
      "3                         NaN    Florida    democrat  15.0   9.0   20.0   \n",
      "4  Wisconsin Assembly speaker  Wisconsin  republican   0.0   3.0    2.0   \n",
      "\n",
      "     0.2   0.3                   a mailer  \n",
      "0    1.0   0.0            a floor speech.  \n",
      "1  163.0   9.0                     Denver  \n",
      "2    5.0  44.0             a news release  \n",
      "3   19.0   2.0        an interview on CNN  \n",
      "4    5.0   1.0  a an online opinion-piece  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['label', 'statement'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Retain only the 'statement' and 'label' columns\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m train_df \u001b[38;5;241m=\u001b[39m train_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     14\u001b[0m val_df \u001b[38;5;241m=\u001b[39m val_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     15\u001b[0m test_df \u001b[38;5;241m=\u001b[39m test_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\tamra\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\tamra\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tamra\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6175\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['label', 'statement'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load LIAR dataset\n",
    "# Update the paths if needed (e.g., \"./data/train.csv\")\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "val_df = pd.read_csv(\"valid.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Just to be safe, print sample rows\n",
    "print(train_df.head())\n",
    "\n",
    "# Retain only the 'statement' and 'label' columns\n",
    "train_df = train_df[['label', 'statement']]\n",
    "val_df = val_df[['label', 'statement']]\n",
    "test_df = test_df[['label', 'statement']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebdf7bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['2635.json', 'FALSE',\n",
      "       'Says the Annies List political group supports third-trimester abortions on demand.',\n",
      "       'abortion', 'dwayne-bohac', 'State representative', 'Texas',\n",
      "       'republican', '0', '1', '0.1', '0.2', '0.3', 'a mailer'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2635.json</th>\n",
       "      <th>FALSE</th>\n",
       "      <th>Says the Annies List political group supports third-trimester abortions on demand.</th>\n",
       "      <th>abortion</th>\n",
       "      <th>dwayne-bohac</th>\n",
       "      <th>State representative</th>\n",
       "      <th>Texas</th>\n",
       "      <th>republican</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>a mailer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12465.json</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>The Chicago Bears have had more starting quart...</td>\n",
       "      <td>education</td>\n",
       "      <td>robin-vos</td>\n",
       "      <td>Wisconsin Assembly speaker</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a an online opinion-piece</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    2635.json        FALSE  \\\n",
       "0  10540.json    half-true   \n",
       "1    324.json  mostly-true   \n",
       "2   1123.json        FALSE   \n",
       "3   9028.json    half-true   \n",
       "4  12465.json         TRUE   \n",
       "\n",
       "  Says the Annies List political group supports third-trimester abortions on demand.  \\\n",
       "0  When did the decline of coal start? It started...                                   \n",
       "1  Hillary Clinton agrees with John McCain \"by vo...                                   \n",
       "2  Health care reform legislation is likely to ma...                                   \n",
       "3  The economic turnaround started at the end of ...                                   \n",
       "4  The Chicago Bears have had more starting quart...                                   \n",
       "\n",
       "                             abortion    dwayne-bohac  \\\n",
       "0  energy,history,job-accomplishments  scott-surovell   \n",
       "1                      foreign-policy    barack-obama   \n",
       "2                         health-care    blog-posting   \n",
       "3                        economy,jobs   charlie-crist   \n",
       "4                           education       robin-vos   \n",
       "\n",
       "         State representative      Texas  republican     0     1    0.1  \\\n",
       "0              State delegate   Virginia    democrat   0.0   0.0    1.0   \n",
       "1                   President   Illinois    democrat  70.0  71.0  160.0   \n",
       "2                         NaN        NaN        none   7.0  19.0    3.0   \n",
       "3                         NaN    Florida    democrat  15.0   9.0   20.0   \n",
       "4  Wisconsin Assembly speaker  Wisconsin  republican   0.0   3.0    2.0   \n",
       "\n",
       "     0.2   0.3                   a mailer  \n",
       "0    1.0   0.0            a floor speech.  \n",
       "1  163.0   9.0                     Denver  \n",
       "2    5.0  44.0             a news release  \n",
       "3   19.0   2.0        an interview on CNN  \n",
       "4    5.0   1.0  a an online opinion-piece  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "print(train_df.columns)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de30571a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\tamra\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimplify_label\u001b[39m(label):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmostly-true\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhalf-true\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(simplify_label)\n\u001b[0;32m      6\u001b[0m val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(simplify_label)\n\u001b[0;32m      7\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(simplify_label)\n",
      "File \u001b[1;32mc:\\Users\\tamra\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\tamra\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "# Mapping multiple classes to binary\n",
    "def simplify_label(label):\n",
    "    return 1 if label in ['true', 'mostly-true', 'half-true'] else 0\n",
    "\n",
    "train_df['label'] = train_df['label'].apply(simplify_label)\n",
    "val_df['label'] = val_df['label'].apply(simplify_label)\n",
    "test_df['label'] = test_df['label'].apply(simplify_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0685fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'id', 'label', 'statement', 'subject', 'speaker', 'job_title',\n",
    "    'state', 'party', 'barely_true_counts', 'false_counts', 'half_true_counts',\n",
    "    'mostly_true_counts', 'pants_on_fire_counts', 'context'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "085740f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\", sep='\\t', names=columns, header=None)\n",
    "val_df = pd.read_csv(\"valid.csv\", sep='\\t', names=columns, header=None)\n",
    "test_df = pd.read_csv(\"test.csv\", sep='\\t', names=columns, header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85f60698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_label(label):\n",
    "    try:\n",
    "        return 1 if str(label).strip().lower() in ['true', 'mostly-true', 'half-true'] else 0\n",
    "    except:\n",
    "        return 0  # Default to 0 if something goes wrong (e.g., NaN)\n",
    "\n",
    "train_df['label'] = train_df['label'].apply(simplify_label)\n",
    "val_df['label'] = val_df['label'].apply(simplify_label)\n",
    "test_df['label'] = test_df['label'].apply(simplify_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5056384b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    10249\n",
      "1       20\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    1284\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    1270\n",
      "1      13\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df['label'].value_counts())\n",
    "print(val_df['label'].value_counts())\n",
    "print(test_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94d66748",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[['label', 'statement']]\n",
    "val_df = val_df[['label', 'statement']]\n",
    "test_df = test_df[['label', 'statement']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "539bfe2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample:\n",
      "    label statement\n",
      "0      0       NaN\n",
      "1      0       NaN\n",
      "2      0       NaN\n",
      "3      0       NaN\n",
      "4      0       NaN\n",
      "\n",
      "Label distribution:\n",
      " label\n",
      "0    10249\n",
      "1       20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define LIAR dataset column names\n",
    "columns = [\n",
    "    'id', 'label', 'statement', 'subject', 'speaker', 'job_title',\n",
    "    'state', 'party', 'barely_true_counts', 'false_counts', 'half_true_counts',\n",
    "    'mostly_true_counts', 'pants_on_fire_counts', 'context'\n",
    "]\n",
    "\n",
    "# Load all three datasets (Update paths if needed)\n",
    "train_df = pd.read_csv(\"train.csv\", sep='\\t', names=columns, header=None)\n",
    "val_df = pd.read_csv(\"valid.csv\", sep='\\t', names=columns, header=None)\n",
    "test_df = pd.read_csv(\"test.csv\", sep='\\t', names=columns, header=None)\n",
    "\n",
    "# Convert multiclass labels to binary: 1 = true/mostly-true/half-true, 0 = everything else\n",
    "def simplify_label(label):\n",
    "    try:\n",
    "        return 1 if str(label).strip().lower() in ['true', 'mostly-true', 'half-true'] else 0\n",
    "    except:\n",
    "        return 0  # fallback in case of NaN or unexpected format\n",
    "\n",
    "train_df['label'] = train_df['label'].apply(simplify_label)\n",
    "val_df['label'] = val_df['label'].apply(simplify_label)\n",
    "test_df['label'] = test_df['label'].apply(simplify_label)\n",
    "\n",
    "# Keep only relevant columns\n",
    "train_df = train_df[['label', 'statement']]\n",
    "val_df = val_df[['label', 'statement']]\n",
    "test_df = test_df[['label', 'statement']]\n",
    "\n",
    "# Print samples to verify\n",
    "print(\"Train sample:\\n\", train_df.head())\n",
    "print(\"\\nLabel distribution:\\n\", train_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0017a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23056fa8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Combine all texts to fit tokenizer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m all_texts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m'\u001b[39m], val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m'\u001b[39m], test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatement\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Tokenization\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine all texts to fit tokenizer\n",
    "all_texts = pd.concat([train_df['statement'], val_df['statement'], test_df['statement']])\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "# Convert to sequences\n",
    "max_len = 40\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_df['statement']), maxlen=max_len)\n",
    "X_val = pad_sequences(tokenizer.texts_to_sequences(val_df['statement']), maxlen=max_len)\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_df['statement']), maxlen=max_len)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_df['label'])\n",
    "y_val = le.transform(val_df['label'])\n",
    "y_test = le.transform(test_df['label'])\n",
    "\n",
    "# Convert to categorical (one-hot)\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train)\n",
    "y_val_cat = tf.keras.utils.to_categorical(y_val)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d683f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b91ee6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement fcmeans (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for fcmeans\n"
     ]
    }
   ],
   "source": [
    "pip install fcmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06077678",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fcmeans'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding, Bidirectional, LSTM, Dense, Dropout\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, accuracy_score\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfcmeans\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FCM\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 2. Combine all text data to fit tokenizer\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fcmeans'"
     ]
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from fcmeans import FCM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 2. Combine all text data to fit tokenizer\n",
    "all_texts = pd.concat([train_df['statement'], val_df['statement'], test_df['statement']])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "# 3. Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df['statement'])\n",
    "X_val_seq = tokenizer.texts_to_sequences(val_df['statement'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['statement'])\n",
    "\n",
    "# 4. Padding\n",
    "max_length = 50\n",
    "X_train = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_val = pad_sequences(X_val_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# 5. Labels\n",
    "y_train = train_df['label'].values\n",
    "y_val = val_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# 6. BiLSTM Model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 7. Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# 8. Evaluate\n",
    "pred_probs = model.predict(X_test)\n",
    "y_pred_initial = (pred_probs > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "print(\"\\nüîç BiLSTM Performance:\")\n",
    "print(classification_report(y_test, y_pred_initial))\n",
    "\n",
    "# 9. üîÅ Fuzzy C-Means Post-processing\n",
    "\n",
    "# Use outputs of last hidden dense layer (64D features)\n",
    "extractor = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "X_test_features = extractor.predict(X_test)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test_features)\n",
    "\n",
    "# Run Fuzzy C-Means clustering\n",
    "fcm = FCM(n_clusters=2)\n",
    "fcm.fit(X_test_scaled)\n",
    "fcm_labels = fcm.predict(X_test_scaled)  # 0 or 1\n",
    "\n",
    "# Determine which cluster is more \"real\" based on majority vote with BiLSTM predictions\n",
    "from scipy.stats import mode\n",
    "real_cluster = mode(fcm_labels[y_pred_initial == 1])[0][0]\n",
    "y_pred_fcm_refined = np.where(fcm_labels == real_cluster, 1, 0)\n",
    "\n",
    "# 10. Final evaluation\n",
    "print(\"\\nüß† After Fuzzy C-Means refinement:\")\n",
    "print(classification_report(y_test, y_pred_fcm_refined))\n",
    "print(\"Initial Accuracy (BiLSTM):\", accuracy_score(y_test, y_pred_initial))\n",
    "print(\"Refined Accuracy (BiLSTM + FCM):\", accuracy_score(y_test, y_pred_fcm_refined))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ea16d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mode\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Use outputs of last hidden dense layer (64D features)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m extractor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39minput, outputs\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39moutput)\n\u001b[0;32m      7\u001b[0m X_test_features \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Normalize features\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47f703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f69b5d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "161/161 [==============================] - 7s 26ms/step - loss: 0.0525 - accuracy: 0.9933 - val_loss: 9.1601e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "161/161 [==============================] - 4s 22ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 1.0917e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "161/161 [==============================] - 4s 22ms/step - loss: 0.0018 - accuracy: 0.9993 - val_loss: 9.3514e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "161/161 [==============================] - 4s 23ms/step - loss: 4.3159e-04 - accuracy: 1.0000 - val_loss: 9.5963e-06 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "161/161 [==============================] - 4s 22ms/step - loss: 9.4741e-05 - accuracy: 1.0000 - val_loss: 6.4361e-06 - val_accuracy: 1.0000\n",
      "41/41 [==============================] - 1s 6ms/step\n",
      "\n",
      "üîç BiLSTM Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1270\n",
      "           1       0.80      0.62      0.70        13\n",
      "\n",
      "    accuracy                           0.99      1283\n",
      "   macro avg       0.90      0.81      0.85      1283\n",
      "weighted avg       0.99      0.99      0.99      1283\n",
      "\n",
      "41/41 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tamra\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\tamra\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=6.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† After KMeans refinement:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1270\n",
      "           1       0.85      0.85      0.85        13\n",
      "\n",
      "    accuracy                           1.00      1283\n",
      "   macro avg       0.92      0.92      0.92      1283\n",
      "weighted avg       1.00      1.00      1.00      1283\n",
      "\n",
      "Initial Accuracy (BiLSTM): 0.9945440374123149\n",
      "Refined Accuracy (BiLSTM + KMeans): 0.9968823070927514\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import mode\n",
    "\n",
    "# # 2. Combine all text data to fit tokenizer\n",
    "# all_texts = pd.concat([train_df['statement'], val_df['statement'], test_df['statement']])\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "# tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "# # 3. Convert text to sequences\n",
    "# X_train_seq = tokenizer.texts_to_sequences(train_df['statement'])\n",
    "# X_val_seq = tokenizer.texts_to_sequences(val_df['statement'])\n",
    "# X_test_seq = tokenizer.texts_to_sequences(test_df['statement'])\n",
    "\n",
    "# 2. Combine all text data to fit tokenizer (ensure strings)\n",
    "all_texts = pd.concat([train_df['statement'], val_df['statement'], test_df['statement']]).astype(str)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "# 3. Convert text to sequences (ensure strings)\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df['statement'].astype(str))\n",
    "X_val_seq = tokenizer.texts_to_sequences(val_df['statement'].astype(str))\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['statement'].astype(str))\n",
    "\n",
    "\n",
    "# 4. Padding\n",
    "max_length = 50\n",
    "X_train = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_val = pad_sequences(X_val_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# 5. Labels\n",
    "y_train = train_df['label'].values\n",
    "y_val = val_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# 6. BiLSTM Model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),\n",
    "    Bidirectional(LSTM(64, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 7. Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# 8. Evaluate\n",
    "pred_probs = model.predict(X_test)\n",
    "y_pred_initial = (pred_probs > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "print(\"\\nüîç BiLSTM Performance:\")\n",
    "print(classification_report(y_test, y_pred_initial))\n",
    "\n",
    "# 9. üîÅ KMeans Post-processing\n",
    "# Use outputs of last hidden dense layer (64D features)\n",
    "extractor = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "X_test_features = extractor.predict(X_test)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test_features)\n",
    "\n",
    "# Run KMeans clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X_test_scaled)\n",
    "\n",
    "# Determine which cluster is more \"real\" based on majority vote with BiLSTM predictions\n",
    "# real_cluster = mode(kmeans_labels[y_pred_initial == 1])[0][0]\n",
    "\n",
    "real_cluster = mode(kmeans_labels[y_pred_initial == 1], keepdims=True).mode[0]\n",
    "y_pred_kmeans_refined = np.where(kmeans_labels == real_cluster, 1, 0)\n",
    "\n",
    "# 10. Final evaluation\n",
    "print(\"\\nüß† After KMeans refinement:\")\n",
    "print(classification_report(y_test, y_pred_kmeans_refined))\n",
    "print(\"Initial Accuracy (BiLSTM):\", accuracy_score(y_test, y_pred_initial))\n",
    "print(\"Refined Accuracy (BiLSTM + KMeans):\", accuracy_score(y_test, y_pred_kmeans_refined))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab42db5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tamra\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 1. Save the BiLSTM model\n",
    "model.save('bilstm_model.h5')\n",
    "\n",
    "# 2. Save the KMeans model\n",
    "with open('kmeans_model.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "\n",
    "# 3. Save the Tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# 4. Save the StandardScaler\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9679cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
